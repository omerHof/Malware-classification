
import torch
import torch.nn as nn
from torch.utils.data import DataLoader,SubsetRandomSampler
import torch.optim as optim
from configs.config_file import Classifier_config,Siamese_net_config
from sklearn.model_selection import KFold
import numpy as np
from tqdm import tqdm
from Models.architectures import classification_model_architecture,siamese_model_architecture
from Data.data_util import Classification_dataset,Triplet_loss_dataset

class Finetune_trained_nets():

    def __init__(self, cfg):
        super().__init__()
        self.cfg = cfg



    def finetune_model(self):
        torch.manual_seed(42)
        splits = KFold(n_splits=self.cfg.finetune_params['kfold'], shuffle=True, random_state=42)
        history = {'train_loss': [], 'test_loss': [], 'train_acc': [], 'test_acc': []}

        for fold, (train_idx, val_idx) in enumerate(splits.split(np.arange(len(self.dataset)))):
            print('Fold {}'.format(fold + 1))

            train_sampler = SubsetRandomSampler(train_idx)
            test_sampler = SubsetRandomSampler(val_idx)
            train_loader = DataLoader(self.dataset, batch_size=self.cfg.loader_params['batch_size'],
                                      sampler=train_sampler, num_workers=self.cfg.loader_params['num_workers'])
            test_loader = DataLoader(self.dataset, batch_size=self.cfg.loader_params['batch_size'],
                                     sampler=test_sampler, num_workers=self.cfg.loader_params['num_workers'])

            model = self.model.model
            optim_params = model.parameters() if mode=='classification' else model.model.model.parameters()
            self.optimizer = optim.Adam(optim_params, lr=0.002)
            lr_scheduler = optim.lr_scheduler.StepLR(self.optimizer, step_size=10, gamma=0.1)
            for epoch in tqdm(range(self.cfg.finetune_params['epochs']),desc='Train net'):
                train_loss, train_correct = self.train_epoch(model, self.device, train_loader, self.cfg.criterion, self.optimizer)
                test_loss, test_correct = self.valid_epoch(model, self.device, test_loader, self.cfg.criterion)

                train_loss = train_loss / len(train_loader.sampler)
                train_acc = train_correct / len(train_loader.sampler) * 100
                test_loss = test_loss / len(test_loader.sampler)
                test_acc = test_correct / len(test_loader.sampler) * 100

                print(
                    "Epoch:{}/{} AVG Training Loss:{:.3f} AVG Test Loss:{:.3f} AVG Training Acc {:.2f} % AVG Test Acc {:.2f} %".format(
                        epoch + 1,
                        self.cfg.finetune_params['epochs'],
                        train_loss,
                        test_loss,
                        train_acc,
                        test_acc))
                history['train_loss'].append(train_loss)
                history['test_loss'].append(test_loss)
                history['train_acc'].append(train_acc)
                history['test_acc'].append(test_acc)
                lr_scheduler.step()


    def train_epoch(self,model, device, dataloader, loss_fn, optimizer):
        train_loss, train_correct = 0.0, 0
        model.train()
        if self.cfg.mode=='classification':
            return self.training_loop_classification(model, device, dataloader, loss_fn, optimizer,train_loss,train_correct)
        else:
            return self.training_loop_embedding(model, device, dataloader, loss_fn, optimizer,train_loss,train_correct)


    def training_loop_classification(self,model, device, dataloader, loss_fn, optimizer,train_loss,train_correct):
        for images, labels in dataloader:
            images, labels = images.to(device), labels.to(device)
            optimizer.zero_grad()
            output = model(images)
            loss = loss_fn(output, labels)
            loss.backward()
            optimizer.step()
            train_loss += loss.item() * images.size(0)
            scores, predictions = torch.max(output.data, 1)
            train_correct += (predictions == labels).sum().item()
        return train_loss, train_correct

    def training_loop_embedding(self, model, device, dataloader, loss_fn, optimizer, train_loss, train_correct):
        for anchor_images,anchor_labels,positive_images,positive_labels, negative_images,negative_labels in dataloader:
            anchor_images,anchor_labels = anchor_images.to(device), anchor_labels.to(device)
            positive_images,positive_labels = positive_images.to(device), positive_labels.to(device)
            negative_images,negative_labels = negative_images.to(device), negative_labels.to(device)
            optimizer.zero_grad()
            output1, output2, output3 = model(anchor_images,positive_images,negative_images)
            loss = loss_fn(output1, output2, output3)
            loss.backward()
            optimizer.step()

            train_loss += loss.item() * anchor_images.size(0)
            # implement here KNN to classify the embedding vector
        return train_loss, train_correct

    def valid_epoch(self,model, device, dataloader, loss_fn):
        valid_loss, val_correct = 0.0, 0
        model.eval()
        if self.cfg.mode == 'classification':
            return self.classification_validation(model, dataloader, device, loss_fn, valid_loss, val_correct)
        else:
            return self.embeddings_validation(model, dataloader, device, loss_fn, valid_loss, val_correct)

    def classification_validation(self, model, dataloader, device, loss_fn, valid_loss, val_correct):
        with torch.no_grad():
            for images, labels in dataloader:
                images, labels = images.to(device), labels.to(device)
                output = model(images)
                loss = loss_fn(output, labels)
                valid_loss += loss.item() * images.size(0)
                scores, predictions = torch.max(output.data, 1)
                val_correct += (predictions == labels).sum().item()
        return valid_loss, val_correct

    def embeddings_validation(self, model, dataloader, device, loss_fn, valid_loss, val_correct):
        with torch.no_grad():
            for anchor_images,anchor_labels,positive_images,positive_labels, negative_images,negative_labels in dataloader:
                anchor_images, anchor_labels = anchor_images.to(device), anchor_labels.to(device)
                positive_images, positive_labels = positive_images.to(device), positive_labels.to(device)
                negative_images, negative_labels = negative_images.to(device), negative_labels.to(device)
                output1, output2, output3 = model(anchor_images, positive_images, negative_images)
                loss = loss_fn(output1, output2, output3)
                valid_loss += loss.item() * anchor_images.size(0)
        return valid_loss, val_correct


class classification_model(Finetune_trained_nets):

    def __init__(self, cfg):
        super(Finetune_trained_nets,self).__init__()
        self.cfg = cfg
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.model = classification_model_architecture(self.cfg)
        self.dataset = Classification_dataset(cfg.root_path)


class siamese_model(Finetune_trained_nets):
    def __init__(self, cfg):
        super(Finetune_trained_nets,self).__init__()
        self.cfg = cfg
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.model = siamese_model_architecture(self.cfg)
        self.dataset = Triplet_loss_dataset(cfg.root_path)


if __name__ == '__main__':
    # mode = 'classification'
    mode = 'siamese'
    if mode=='classification':
        cfg = Classifier_config()
        ftn = classification_model(cfg)
    else:
        cfg = Siamese_net_config()
        ftn = siamese_model(cfg)
    ftn.finetune_model()