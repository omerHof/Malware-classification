
import torch
import torch.nn as nn
from torch.utils.data import DataLoader,SubsetRandomSampler
import torch.optim as optim
from configs.config_file import Classifier_config,Siamese_net_config
from sklearn.model_selection import KFold
import numpy as np
import seaborn as sns
from tqdm import tqdm
from tqdm.notebook import tqdm as tqdm1
from Models.architectures import classification_model_architecture,siamese_model_architecture
from Data.data_util import Classification_dataset,Triplet_loss_dataset,MALWARE_CLASSES
import matplotlib.pyplot as plt
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
import pandas as pd
from sklearn.metrics import precision_recall_fscore_support
from sklearn import datasets, metrics, model_selection, svm
from sklearn.metrics import roc_auc_score
import imutils
from sklearn.metrics import (
    confusion_matrix,
    accuracy_score,
    precision_score,
    recall_score,
    f1_score
)

class Finetune_trained_nets():
    def __init__(self, cfg):
        super().__init__()
        self.cfg = cfg

    def finetune_model(self):
        torch.manual_seed(42)
        splits = KFold(n_splits=self.cfg.finetune_params['kfold'], shuffle=True, random_state=42)
        history = {'train_loss': [], 'test_loss': [], 'train_acc': [], 'test_acc': []}

        for fold, (train_idx, val_idx) in enumerate(splits.split(np.arange(len(self.dataset)))):
            print('Fold {}'.format(fold + 1))
            if self.cfg.mode == 'siamese':
                train_siamese, valid_siamese, train_ml = np.split(train_idx, np.cumsum(np.floor(np.multiply([0.6, 0.1, 0.3], len(train_idx))).astype(int))[:-1])
                train_siamese_sampler = SubsetRandomSampler(train_siamese)
                valid_siamese_sampler = SubsetRandomSampler(valid_siamese)
                train_ml_sampler = SubsetRandomSampler(train_ml)
                test_sampler = SubsetRandomSampler(val_idx)
                train_loader = DataLoader(self.dataset, batch_size=self.cfg.loader_params['batch_size'],
                                          sampler=train_siamese_sampler, num_workers=self.cfg.loader_params['num_workers'])
                test_loader = DataLoader(self.dataset, batch_size=self.cfg.loader_params['batch_size'],
                                         sampler=valid_siamese_sampler, num_workers=self.cfg.loader_params['num_workers'])
                train_ml_loader = DataLoader(self.dataset, batch_size=self.cfg.loader_params['batch_size'],
                                          sampler=train_ml_sampler, num_workers=self.cfg.loader_params['num_workers'])
                test_ml_loader = DataLoader(self.dataset, batch_size=self.cfg.loader_params['batch_size'],
                                         sampler=test_sampler, num_workers=self.cfg.loader_params['num_workers'])
            else:
                train_sampler = SubsetRandomSampler(train_idx)
                test_sampler = SubsetRandomSampler(val_idx)
                train_loader = DataLoader(self.dataset, batch_size=self.cfg.loader_params['batch_size'],
                                          sampler=train_sampler, num_workers=self.cfg.loader_params['num_workers'])
                test_loader = DataLoader(self.dataset, batch_size=self.cfg.loader_params['batch_size'],
                                         sampler=test_sampler, num_workers=self.cfg.loader_params['num_workers'])

            model = self.model.model
            # optim_params = model.parameters() if mode=='classification' else model.model.model.parameters()
            optim_params = model.parameters()
            self.optimizer = optim.Adam(optim_params, lr=0.0001)
            lr_scheduler = optim.lr_scheduler.StepLR(self.optimizer, step_size=10, gamma=0.1)
            for epoch in tqdm(range(self.cfg.finetune_params['epochs']),desc='Train net'):
                if self.cfg.mode == 'siamese':
                    train_loss, train_correct, optimizer, model = self.train_epoch(model, self.device, train_loader, self.cfg.criterion, self.optimizer)
                    test_loss, test_correct = self.valid_epoch(model, self.device, test_loader, self.cfg.criterion)
                else:
                    train_loss, train_correct, optimizer, model = self.train_epoch(model, self.device, train_loader, self.cfg.criterion, self.optimizer)
                    test_loss, test_correct = self.valid_epoch(model, self.device, test_loader, self.cfg.criterion)

                train_loss = train_loss / len(train_loader.sampler)
                train_acc = train_correct / len(train_loader.sampler) * 100
                test_loss = test_loss / len(test_loader.sampler)
                test_acc = test_correct / len(test_loader.sampler) * 100


                print(
                    "Epoch:{}/{} AVG Training Loss:{:.3f} AVG Test Loss:{:.3f} AVG Training Acc {:.2f} % AVG Test Acc {:.2f} %".format(
                        epoch + 1,
                        self.cfg.finetune_params['epochs'],
                        train_loss,
                        test_loss,
                        train_acc,
                        test_acc))
                history['train_loss'].append(train_loss)
                history['test_loss'].append(test_loss)
                history['train_acc'].append(train_acc)
                history['test_acc'].append(test_acc)
                lr_scheduler.step()
                if epoch>2:
                    self.train_loss_visalization(history)

                if self.cfg.mode=='siamese':
                    self.training_visualization(model,train_loader, fold, epoch)
                #     # self.siamese_ml_model(model, train_ml_loader, test_ml_loader, fold, epoch)
                #     # self.siamese_ml_model(model, train_ml_loader, test_ml_loader, fold, epoch)
            if self.cfg.mode=='siamese':
                self.save_params(model, optimizer)
                accuracy = self.siamese_ml_model(model, train_ml_loader, test_ml_loader, fold, -1)
                print(accuracy)

    def save_params(self, model, optimizer):
        print("save_params")
        torch.save({"model_state_dict": model.state_dict(),
                    "optimzier_state_dict": optimizer.state_dict()
                    }, "trained_model.pth")

    def training_visualization(self,siamese_model,train_loader, fold, epoch):
        train_embedding, train_labels = self.create_embedding_lists(train_loader,siamese_model)
        self.visualization_PCA(train_embedding, train_labels, fold, epoch)

    def create_embedding_lists(self,data_loader,siamese_model):
        embeddings = []
        lables = []
        with torch.no_grad():
            for anchor_images, anchor_labels, positive_images, positive_labels, negative_images, negative_labels in data_loader:
                anchor_embed, positive_embed, negative_embed = siamese_model(anchor_images.to(self.device), positive_images.to(self.device), negative_images.to(self.device))

                embeddings.append(anchor_embed.cpu().numpy())
                embeddings.append(positive_embed.cpu().numpy())
                embeddings.append(negative_embed.cpu().numpy())
                lables.append(anchor_labels.cpu().numpy())
                lables.append(positive_labels.cpu().numpy())
                lables.append(negative_labels.cpu().numpy())
        return np.vstack(embeddings), np.hstack(lables)


    def siamese_ml_model(self, siamese_model, train_loader, test_loader, fold, epoch):
        # model return the embedding from the images
        # model.eval()


        train_embedding, train_labels = self.create_embedding_lists(train_loader,siamese_model)
        test_embedding, test_labels = self.create_embedding_lists(test_loader,siamese_model)
        # self.visualization_TSNE(train_embedding, train_labels, fold, epoch)

        # if step == "final":
        #     self.visualization2(train_results, train_labels)
        # test_labels = c(test_labels)
        self.ml_model(train_embedding, train_labels, test_embedding, test_labels)


    def ml_model(self, train_images, train_labels, test_images, test_labels):
        from sklearn.neural_network import MLPClassifier
        from sklearn.preprocessing import LabelEncoder

        # model = MLPClassifier(hidden_layer_sizes=(16, 16, 16), activation='relu', solver='adam', max_iter=1000)
        model = XGBClassifier(seed=2020)
        le = LabelEncoder()
        train_labels = le.fit_transform(train_labels)

        model.fit(pd.DataFrame(train_images), pd.Series(train_labels))
        predictions = model.predict(pd.DataFrame(test_images))
        predictions = le.inverse_transform(predictions)
        accuracy = accuracy_score(test_labels, predictions)
        precision = precision_score(test_labels, predictions, average='macro')
        recall = recall_score(test_labels, predictions, average='macro')
        f1score = f1_score(test_labels, predictions, average='macro')

        print("Precision = ", precision)
        print("Recall = ", recall)
        print("F1 Score = ", f1score)
        print("Accuracy = ", accuracy)

        # print(precision_recall_fscore_support(test_labels, predictions))
        # metrics.plot_roc_curve(model, test_images, test_labels, pos_label=1)
        # self.curve(model, test_images, test_labels, fold, epoch)
        print("roc score", roc_auc_score(test_labels, model.predict_proba(test_images), multi_class='ovr'))



    def curve(self, model, X_test, y_test, fold, epoch):
        y_pred_proba = model.predict_proba(X_test)[::, 1]
        fpr, tpr, _ = metrics.roc_curve(y_test, y_pred_proba)
        auc = metrics.roc_auc_score(y_test, y_pred_proba)
        plt.plot(fpr, tpr, label="data 1, auc=" + str(auc))
        plt.legend(loc=4)
        plt.savefig("auc_curve_" + str(fold + 1) + "_" + str(epoch) + ".png")
        plt.show()


    def visualization(self, train_results, labels, fold, epoch):
        from mpl_toolkits.mplot3d import Axes3D
        print("visualization_"+str(fold+1)+"_"+str(epoch))
        # plt.figure(figsize=(15, 10), facecolor="azure")
        fig = plt.figure(figsize=(15, 10), facecolor="azure")
        ax = fig.add_subplot(111, projection='3d')
        for label in np.unique(labels):
            tmp = train_results[labels == label]
            plt.scatter(tmp[:, 0], tmp[:, 1], tmp[:, 2], label=label, alpha=0.7)

        # plt.legend()
        ax.legend()
        ax.set_xlabel('X')
        ax.set_ylabel('Y')
        ax.set_zlabel('Z')
        plt.savefig("siamese_"+str(fold+1)+"_"+str(epoch)+".png")
        plt.show()

    def visualization_PCA(self, train_results, labels, fold, epoch):
        # Flatten the image array to (num_samples, num_features)
        print("visualization_" + str(fold + 1) + "_" + str(epoch))
        num_samples = train_results.shape[0]
        num_features = np.prod(train_results.shape[1:])
        train_images_flattened = train_results.reshape(num_samples, num_features)

        # Perform PCA for dimensionality reduction to 2D
        pca = PCA(n_components=2)
        train_images_2d = pca.fit_transform(train_images_flattened)

        # Define the class labels and corresponding colors
        class_labels = MALWARE_CLASSES  # Define the labels according to your dataset
        class_colors = ["red", "green", "blue", "pink", "brown", "yellow","orange", "grey", "purple"]  # Define the colors for each class

        # Plotting the data points
        fig = plt.figure(figsize=(8, 8))

        for i in range(len(class_labels)):
            # Filter the data points belonging to the current class
            class_indices = np.where(labels == i)[0]
            class_points = train_images_2d[class_indices]

            # Scatter plot the data points with the corresponding color
            plt.scatter(class_points[:, 0], class_points[:, 1], color=class_colors[i], label=class_labels[i])

        plt.xlabel("Principal Component 1")
        plt.ylabel("Principal Component 2")
        plt.title("Embeddings Visualization")
        plt.legend()
        plt.savefig("/sise/home/omerhof/malware-classification/images/siamese_"+str(fold+1)+"_"+str(epoch)+".png")
        # plt.show()

    def visualization_TSNE(self,train_results, labels, fold, epoch):
        tsne = TSNE(n_components=2, random_state=0)
        train_images_2d = tsne.fit_transform(train_results)
        target_ids = range(len(MALWARE_CLASSES))
        class_colors = ["red", "green", "blue", "pink", "brown", "yellow","orange", "grey", "purple"]  # Define the colors for each class
        for i, c, label in zip(target_ids, class_colors, labels):
            plt.scatter(train_images_2d[labels == i, 0], train_images_2d[labels == i, 1], c=c, label=label)
        plt.legend()

        plt.xlabel("X")
        plt.ylabel("Y")
        plt.title("Embeddings Visualization")
        plt.legend()
        plt.savefig(
            "/sise/home/omerhof/malware-classification/images/siamese_" + str(fold + 1) + "_" + str(epoch) + ".png")
        plt.clf()


    def train_epoch(self,model, device, dataloader, loss_fn, optimizer):
        train_loss, train_correct = 0.0, 0
        model.train()
        if self.cfg.mode=='classification':
            return self.training_loop_classification(model, device, dataloader, loss_fn, optimizer,train_loss,train_correct)
        else:
            return self.training_loop_embedding(model, device, dataloader, loss_fn, optimizer,train_loss,train_correct)


    def training_loop_classification(self,model, device, dataloader, loss_fn, optimizer,train_loss,train_correct):
        for images, labels in dataloader:
            images, labels = images.to(device), labels.to(device)
            optimizer.zero_grad()
            output = model(images)
            loss = loss_fn(output, labels)
            loss.backward()
            optimizer.step()
            train_loss += loss.item() * images.size(0)
            scores, predictions = torch.max(output.data, 1)
            train_correct += (predictions == labels).sum().item()
        return train_loss, train_correct, optimizer, model

    def training_loop_embedding(self, model, device, dataloader, loss_fn, optimizer, train_loss, train_correct):
        for anchor_images,anchor_labels,positive_images,positive_labels, negative_images,negative_labels in dataloader:
            anchor_images,anchor_labels = anchor_images.to(device), anchor_labels.to(device)
            positive_images,positive_labels = positive_images.to(device), positive_labels.to(device)
            negative_images,negative_labels = negative_images.to(device), negative_labels.to(device)
            optimizer.zero_grad()
            output1, output2, output3 = model(anchor_images,positive_images,negative_images)
            loss = loss_fn(output1, output2, output3)
            loss.backward()
            optimizer.step()

            train_loss += loss.item() #* anchor_images.size(0)
            # implement here KNN to classify the embedding vector
        return train_loss, train_correct, optimizer, model

    def valid_epoch(self,model, device, dataloader, loss_fn):
        valid_loss, val_correct = 0.0, 0
        model.eval()
        if self.cfg.mode == 'classification':
            return self.classification_validation(model, dataloader, device, loss_fn, valid_loss, val_correct)
        else:
            return self.embeddings_validation(model, dataloader, device, loss_fn, valid_loss, val_correct)

    def classification_validation(self, model, dataloader, device, loss_fn, valid_loss, val_correct):
        with torch.no_grad():
            for images, labels in dataloader:
                images, labels = images.to(device), labels.to(device)
                output = model(images)
                loss = loss_fn(output, labels)
                valid_loss += loss.item() * images.size(0)
                scores, predictions = torch.max(output.data, 1)
                val_correct += (predictions == labels).sum().item()
        return valid_loss, val_correct

    def embeddings_validation(self, model, dataloader, device, loss_fn, valid_loss, val_correct):
        with torch.no_grad():
            for anchor_images,anchor_labels,positive_images,positive_labels, negative_images,negative_labels in dataloader:
                anchor_images, anchor_labels = anchor_images.to(device), anchor_labels.to(device)
                positive_images, positive_labels = positive_images.to(device), positive_labels.to(device)
                negative_images, negative_labels = negative_images.to(device), negative_labels.to(device)
                output1, output2, output3 = model(anchor_images, positive_images, negative_images)
                loss = loss_fn(output1, output2, output3)
                valid_loss += loss.item() #* anchor_images.size(0)
        return valid_loss, val_correct

    def train_loss_visalization(self, history):
        epochs = np.arange(1, len(history['train_loss'])+1)
        df = pd.DataFrame({
            'epoch': np.concatenate([epochs, epochs]),
            'loss': np.concatenate([history['train_loss'], history['test_loss']]),
            'type': ['train'] * len(history['train_loss']) + ['val'] * len(history['test_loss'])
        })

        # Use seaborn to create the plot
        plt.figure(figsize=(10, 6))
        sns.lineplot(x='epoch', y='loss', hue='type', data=df)
        plt.title('Training and Validation Loss Convergence')
        plt.xlabel('Epoch')
        plt.ylabel('Loss')
        # plt.show()
        plt.savefig(
            f"/sise/home/omerhof/malware-classification/images/loss_converges_epoch_{len(history['train_loss'])}.png")
        plt.clf()



class classification_model(Finetune_trained_nets):

    def __init__(self, cfg):
        super(Finetune_trained_nets,self).__init__()
        self.cfg = cfg
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.model = classification_model_architecture(self.cfg)
        self.dataset = Classification_dataset(cfg.root_path)


class siamese_model(Finetune_trained_nets):
    def __init__(self, cfg):
        super(Finetune_trained_nets,self).__init__()
        self.cfg = cfg
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

        self.model = siamese_model_architecture(self.cfg)
        self.dataset = Triplet_loss_dataset(cfg.root_path)


if __name__ == '__main__':
    print('check')
    # mode = 'classification'
    mode = 'siamese'
    if mode=='classification':
        cfg = Classifier_config()
        ftn = classification_model(cfg)
    else:
        cfg = Siamese_net_config()
        ftn = siamese_model(cfg)
    ftn.finetune_model()
