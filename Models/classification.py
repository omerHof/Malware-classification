
import torch
import torch.nn as nn
from torch.utils.data import DataLoader,SubsetRandomSampler
import torch.optim as optim
from configs.config_file import Classifier_config,Siamese_net_config
from sklearn.model_selection import KFold
import numpy as np
import seaborn as sns
from tqdm import tqdm
from tqdm.notebook import tqdm as tqdm1
from Models.architectures import classification_model_architecture,siamese_model_architecture
from Data.data_util import Classification_dataset,Triplet_loss_dataset,MALWARE_CLASSES
import matplotlib.pyplot as plt
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
import pandas as pd
from sklearn.metrics import precision_recall_fscore_support
from sklearn import datasets, metrics, model_selection, svm
from sklearn.metrics import roc_auc_score, classification_report
from sklearn.preprocessing import label_binarize
from sklearn.preprocessing import LabelBinarizer

from sklearn.metrics import confusion_matrix
import seaborn as sns
import warnings
warnings.filterwarnings('always')

import imutils
from sklearn.metrics import (
    confusion_matrix,
    accuracy_score,
    precision_score,
    recall_score,
    f1_score
)

ID=1

class Finetune_trained_nets():
    def __init__(self, cfg):
        super().__init__()
        self.cfg = cfg

    def finetune_model(self):
        torch.manual_seed(42)
        splits = KFold(n_splits=self.cfg.finetune_params['kfold'], shuffle=True, random_state=42)
        history = {'train_loss': [], 'test_loss': [], 'train_acc': [], 'test_acc': []}
        precision_list, recall_list, f1score_list, accuracy_list, roc_score_list = [], [], [], [], []
        for fold, (train_idx, val_idx) in enumerate(splits.split(np.arange(len(self.dataset)))):
            print('Fold {}'.format(fold + 1))
            print('ID:', ID)
            if self.cfg.mode == 'siamese':
                np.random.shuffle(train_idx)
                train_siamese, valid_siamese, train_ml = np.split(train_idx, np.cumsum(np.floor(np.multiply([0.2, 0.05, 0.75], len(train_idx))).astype(int))[:-1])
                train_siamese_sampler = SubsetRandomSampler(train_siamese)
                valid_siamese_sampler = SubsetRandomSampler(valid_siamese)
                train_ml_sampler = SubsetRandomSampler(train_ml)
                test_sampler = SubsetRandomSampler(val_idx)
                train_loader = DataLoader(self.dataset, batch_size=self.cfg.loader_params['batch_size'],
                                          sampler=train_siamese_sampler, num_workers=self.cfg.loader_params['num_workers'])
                test_loader = DataLoader(self.dataset, batch_size=self.cfg.loader_params['batch_size'],
                                         sampler=valid_siamese_sampler, num_workers=self.cfg.loader_params['num_workers'])
                train_ml_loader = DataLoader(self.dataset, batch_size=self.cfg.loader_params['batch_size'],
                                          sampler=train_ml_sampler, num_workers=self.cfg.loader_params['num_workers'])
                test_ml_loader = DataLoader(self.dataset, batch_size=self.cfg.loader_params['batch_size'],
                                         sampler=test_sampler, num_workers=self.cfg.loader_params['num_workers'])
            else:
                train_sampler = SubsetRandomSampler(train_idx)
                test_sampler = SubsetRandomSampler(val_idx)
                train_loader = DataLoader(self.dataset, batch_size=self.cfg.loader_params['batch_size'],
                                          sampler=train_sampler, num_workers=self.cfg.loader_params['num_workers'])
                test_loader = DataLoader(self.dataset, batch_size=self.cfg.loader_params['batch_size'],
                                         sampler=test_sampler, num_workers=self.cfg.loader_params['num_workers'])

            model = self.model.model
            # optim_params = model.parameters() if mode=='classification' else model.model.model.parameters()
            optim_params = model.parameters()
            self.optimizer = optim.Adam(optim_params, lr=0.001)
            lr_scheduler = optim.lr_scheduler.StepLR(self.optimizer, step_size=10, gamma=0.1)
            for epoch in tqdm(range(self.cfg.finetune_params['epochs']),desc='Train net'):
                if self.cfg.mode == 'siamese':
                    train_loss, train_correct, optimizer, model = self.train_epoch(model, self.device, train_loader, self.cfg.criterion, self.optimizer)
                    test_loss, test_correct = self.valid_epoch(model, self.device, test_loader, self.cfg.criterion)
                else:
                    train_loss, train_correct, optimizer, model, precision, recall, f1_score, auc_score = self.train_epoch(model, self.device, train_loader, self.cfg.criterion, self.optimizer)
                    test_loss, test_correct = self.valid_epoch(model, self.device, test_loader, self.cfg.criterion)

                train_loss = train_loss / len(train_loader.sampler)
                train_acc = train_correct / len(train_loader.sampler) * 100
                test_loss = test_loss / len(test_loader.sampler)
                test_acc = test_correct / len(test_loader.sampler) * 100


                print(
                    "Epoch:{}/{} AVG Training Loss:{:.3f} AVG Test Loss:{:.3f} AVG Training Acc {:.2f} % AVG Test Acc {:.2f} %".format(
                        epoch + 1,
                        self.cfg.finetune_params['epochs'],
                        train_loss,
                        test_loss,
                        train_acc,
                        test_acc))
                history['train_loss'].append(train_loss)
                history['test_loss'].append(test_loss)
                history['train_acc'].append(train_acc)
                history['test_acc'].append(test_acc)
                lr_scheduler.step()

            if self.cfg.mode == 'classification' and epoch==4:
                precision_list.append(precision)
                recall_list.append(recall)
                f1score_list.append(f1_score)
                accuracy_list.append(test_acc)
                roc_score_list.append(auc_score)

            if self.cfg.mode=='siamese':
                self.save_params(model, optimizer)
                precision, recall, f1score, accuracy, roc_score = self.siamese_ml_model(model, train_ml_loader, test_ml_loader, fold, -1)
                precision_list.append(precision)
                recall_list.append(recall)
                f1score_list.append(f1score)
                accuracy_list.append(accuracy)
                roc_score_list.append(roc_score)

        print("Final results:")
        print("precision:", np.mean(precision_list))
        print("recall:", np.mean(recall_list))
        print("f1score:", np.mean(f1score_list))
        print("accuracy:", np.mean(accuracy_list))
        print("roc_score:", np.mean(roc_score_list))


    def save_params(self, model, optimizer):
        print("save_params")
        torch.save({"model_state_dict": model.state_dict(),
                    "optimzier_state_dict": optimizer.state_dict()
                    }, "trained_model.pth")

    def training_visualization(self,siamese_model,train_loader, fold, epoch):
        train_embedding, train_labels = self.create_embedding_lists(train_loader,siamese_model)
        self.visualization_PCA(train_embedding, train_labels, fold, epoch)

    def create_embedding_lists(self,data_loader,siamese_model):
        embeddings = []
        lables = []
        with torch.no_grad():
            for anchor_images, anchor_labels, positive_images, positive_labels, negative_images, negative_labels in data_loader:
                anchor_embed, positive_embed, negative_embed = siamese_model(anchor_images.to(self.device), positive_images.to(self.device), negative_images.to(self.device))

                embeddings.append(anchor_embed.cpu().numpy())
                embeddings.append(positive_embed.cpu().numpy())
                embeddings.append(negative_embed.cpu().numpy())
                lables.append(anchor_labels.cpu().numpy())
                lables.append(positive_labels.cpu().numpy())
                lables.append(negative_labels.cpu().numpy())
        return np.vstack(embeddings), np.hstack(lables)


    def siamese_ml_model(self, siamese_model, train_loader, test_loader, fold, epoch):
        train_embedding, train_labels = self.create_embedding_lists(train_loader,siamese_model)
        test_embedding, test_labels = self.create_embedding_lists(test_loader,siamese_model)
        # self.visualization_TSNE(train_embedding, train_labels, fold, epoch)

        # if step == "final":
        #     self.visualization2(train_results, train_labels)
        # test_labels = c(test_labels)
        return self.ml_model(train_embedding, train_labels, test_embedding, test_labels)


    def confusion_matrix_visualizaion(self, test_labels, predictions):
        cm = confusion_matrix(test_labels, predictions)
        cm_df = pd.DataFrame(cm, index=MALWARE_CLASSES, columns=MALWARE_CLASSES)

        plt.figure(figsize=(10, 8))
        sns.heatmap(cm_df, annot=True)
        plt.title('Confusion Matrix')
        plt.ylabel('Actal Values')
        plt.xlabel('Predicted Values')
        plt.savefig("/sise/home/szoke/malware-classification/Models/images/confusion_matrix_"+str(ID)+".png")
        plt.show()

    def ml_model(self, train_images, train_labels, test_images, test_labels):
        from sklearn.preprocessing import LabelEncoder
        from sklearn.ensemble import RandomForestClassifier
        from sklearn.neighbors import KNeighborsClassifier
        from sklearn.naive_bayes import GaussianNB
        from sklearn.svm import SVC
        # model = RandomForestClassifier()
        # model = KNeighborsClassifier()
        model = GaussianNB()
        # model = SVC(probability=True)
        # model = XGBClassifier(seed=2020)
        le = LabelEncoder()
        train_labels = le.fit_transform(train_labels)

        model.fit(pd.DataFrame(train_images), pd.Series(train_labels))
        predictions = model.predict(pd.DataFrame(test_images))
        predictions = le.inverse_transform(predictions)
        accuracy = accuracy_score(test_labels, predictions)
        precision = precision_score(test_labels, predictions, average='macro')
        recall = recall_score(test_labels, predictions, average='macro')
        f1score = f1_score(test_labels, predictions, average='macro')

        print("Precision = ", precision)
        print("Recall = ", recall)
        print("F1 Score = ", f1score)
        print("Accuracy = ", accuracy)

        # print(precision_recall_fscore_support(test_labels, predictions))
        # metrics.plot_roc_curve(model, test_images, test_labels, pos_label=1)
        # self.curve(model, test_images, test_labels, fold, epoch)
        roc_score = roc_auc_score(test_labels, model.predict_proba(test_images), multi_class='ovr')
        print("roc score", roc_score)
        self.confusion_matrix_visualizaion(test_labels, predictions)
        return precision, recall, f1score, accuracy, roc_score



    def curve(self, model, X_test, y_test, fold, epoch):
        y_pred_proba = model.predict_proba(X_test)[::, 1]
        fpr, tpr, _ = metrics.roc_curve(y_test, y_pred_proba)
        auc = metrics.roc_auc_score(y_test, y_pred_proba)
        plt.plot(fpr, tpr, label="data 1, auc=" + str(auc))
        plt.legend(loc=4)
        plt.savefig("auc_curve_" + str(fold + 1) + "_" + str(epoch) + ".png")
        plt.show()


    def visualization(self, train_results, labels, fold, epoch):
        from mpl_toolkits.mplot3d import Axes3D
        print("visualization_"+str(fold+1)+"_"+str(epoch))
        # plt.figure(figsize=(15, 10), facecolor="azure")
        fig = plt.figure(figsize=(15, 10), facecolor="azure")
        ax = fig.add_subplot(111, projection='3d')
        for label in np.unique(labels):
            tmp = train_results[labels == label]
            plt.scatter(tmp[:, 0], tmp[:, 1], tmp[:, 2], label=label, alpha=0.7)

        # plt.legend()
        ax.legend()
        ax.set_xlabel('X')
        ax.set_ylabel('Y')
        ax.set_zlabel('Z')
        plt.savefig("siamese_"+str(fold+1)+"_"+str(epoch)+".png")
        plt.show()

    def visualization_PCA(self, train_results, labels, fold, epoch):
        # Flatten the image array to (num_samples, num_features)
        print("visualization_" + str(fold + 1) + "_" + str(epoch))
        num_samples = train_results.shape[0]
        num_features = np.prod(train_results.shape[1:])
        train_images_flattened = train_results.reshape(num_samples, num_features)

        # Perform PCA for dimensionality reduction to 2D
        pca = PCA(n_components=2)
        train_images_2d = pca.fit_transform(train_images_flattened)

        # Define the class labels and corresponding colors
        class_labels = MALWARE_CLASSES  # Define the labels according to your dataset
        class_colors = ["red", "green", "blue", "pink", "brown", "yellow","orange", "grey", "purple"]  # Define the colors for each class

        # Plotting the data points
        fig = plt.figure(figsize=(8, 8))

        for i in range(len(class_labels)):
            # Filter the data points belonging to the current class
            class_indices = np.where(labels == i)[0]
            class_points = train_images_2d[class_indices]

            # Scatter plot the data points with the corresponding color
            plt.scatter(class_points[:, 0], class_points[:, 1], color=class_colors[i], label=class_labels[i])

        plt.xlabel("Principal Component 1")
        plt.ylabel("Principal Component 2")
        plt.title("Embeddings Visualization")
        plt.legend()
        plt.savefig("/sise/home/omerhof/malware-classification/images/siamese_"+str(fold+1)+"_"+str(epoch)+".png")
        # plt.show()

    def visualization_TSNE(self,train_results, labels, fold, epoch):
        tsne = TSNE(n_components=2, random_state=0)
        train_images_2d = tsne.fit_transform(train_results)
        target_ids = range(len(MALWARE_CLASSES))
        class_colors = ["red", "green", "blue", "pink", "brown", "yellow","orange", "grey", "purple"]  # Define the colors for each class
        for i, c, label in zip(target_ids, class_colors, labels):
            plt.scatter(train_images_2d[labels == i, 0], train_images_2d[labels == i, 1], c=c, label=label)
        plt.legend()

        plt.xlabel("X")
        plt.ylabel("Y")
        plt.title("Embeddings Visualization")
        plt.legend()
        plt.savefig(
            "/sise/home/omerhof/malware-classification/images/siamese_" + str(fold + 1) + "_" + str(epoch) + ".png")
        plt.clf()


    def train_epoch(self,model, device, dataloader, loss_fn, optimizer):
        train_loss, train_correct = 0.0, 0
        model.train()
        if self.cfg.mode=='classification':
            return self.training_loop_classification(model, device, dataloader, loss_fn, optimizer,train_loss,train_correct)
        else:
            return self.training_loop_embedding(model, device, dataloader, loss_fn, optimizer,train_loss,train_correct)

    # function for scoring roc auc score for multi-class
    def multiclass_roc_auc_score(self, y_test, y_pred, average="macro"):
        lb = LabelBinarizer()
        lb.fit(y_test)
        y_test = lb.transform(y_test)
        y_pred = lb.transform(y_pred)
        return roc_auc_score(y_test, y_pred, average=average)


    def training_loop_classification(self,model, device, dataloader, loss_fn, optimizer,train_loss,train_correct):
        precision = 0
        recall = 0
        f1 = 0
        auc_score = 0
        labels_list = []
        preds_list = []
        for images, labels in dataloader:
            images, labels = images.to(device), labels.to(device)
            optimizer.zero_grad()
            output = model(images)
            loss = loss_fn(output, labels)
            # loss.requires_grad = True
            loss.backward()
            optimizer.step()
            train_loss += loss.item() * images.size(0)
            scores, predictions = torch.max(output.data, 1)

            labels_list.append(labels)
            preds_list.append(predictions)
            train_correct += (predictions == labels).sum().item()
        precision += precision_score(labels.cpu().numpy(), predictions.cpu().numpy(), average='macro')
        recall += recall_score(labels.cpu().numpy(), predictions.cpu().numpy(), average='macro')
        f1 += f1_score(labels.cpu().numpy(), predictions.cpu().numpy(), average='macro')
        auc_score = self.multiclass_roc_auc_score(labels.cpu().numpy(), predictions.cpu().numpy())

        return train_loss, train_correct, optimizer, model, precision, recall, f1, auc_score

    import torch
    import numpy as np
    from sklearn.metrics import roc_auc_score

    # def training_loop_classification(self, model, device, dataloader, loss_fn, optimizer, train_loss, train_correct):
    #     num_classes = len(MALWARE_CLASSES)
    #     true_positives = np.zeros(num_classes)
    #     false_positives = np.zeros(num_classes)
    #     false_negatives = np.zeros(num_classes)
    #     all_probs = []
    #     all_labels = []
    #
    #     for images, labels in dataloader:
    #         images, labels = images.to(device), labels.to(device)
    #         optimizer.zero_grad()
    #         output = model(images)
    #         loss = loss_fn(output, labels)
    #         loss.requires_grad = True
    #         loss.backward()
    #         optimizer.step()
    #         train_loss += loss.item() * images.size(0)
    #         _, predictions = torch.max(output.data, 1)
    #         train_correct += (predictions == labels).sum().item()
    #
    #         # Apply softmax to obtain probability scores
    #         probs = torch.softmax(output, dim=1)
    #         all_probs.extend(probs.tolist())
    #         all_labels.extend(labels.tolist())
    #
    #         # Calculate true positives, false positives, false negatives for each class
    #         for class_idx in range(num_classes):
    #             true_positives[class_idx] += ((predictions == class_idx) & (labels == class_idx)).sum().item()
    #             false_positives[class_idx] += ((predictions == class_idx) & (labels != class_idx)).sum().item()
    #             false_negatives[class_idx] += ((predictions != class_idx) & (labels == class_idx)).sum().item()
    #
    #     precision = true_positives / (
    #                 true_positives + false_positives + 1e-8)  # Adding epsilon to avoid division by zero
    #     recall = true_positives / (true_positives + false_negatives + 1e-8)
    #     f1_score = 2 * (precision * recall) / (precision + recall + 1e-8)
    #
    #     # Convert probability scores and labels to numpy arrays for roc_auc_score
    #     all_probs = np.array(all_probs)
    #     all_labels = np.array(all_labels)
    #
    #     # roc_score = roc_auc_score(all_labels, all_probs, multi_class='ovr')  # One-vs-Rest ROC score
    #
    #     return train_loss, train_correct, optimizer, model, precision, recall, f1_score, 0

    def training_loop_embedding(self, model, device, dataloader, loss_fn, optimizer, train_loss, train_correct):
        for anchor_images,anchor_labels,positive_images,positive_labels, negative_images,negative_labels in dataloader:
            anchor_images,anchor_labels = anchor_images.to(device), anchor_labels.to(device)
            positive_images,positive_labels = positive_images.to(device), positive_labels.to(device)
            negative_images,negative_labels = negative_images.to(device), negative_labels.to(device)
            optimizer.zero_grad()
            output1, output2, output3 = model(anchor_images,positive_images,negative_images)
            loss = loss_fn(output1, output2, output3)
            loss.backward()
            optimizer.step()

            train_loss += loss.item() #* anchor_images.size(0)
            # implement here KNN to classify the embedding vector
        return train_loss, train_correct, optimizer, model

    def valid_epoch(self,model, device, dataloader, loss_fn):
        valid_loss, val_correct = 0.0, 0
        model.eval()
        if self.cfg.mode == 'classification':
            return self.classification_validation(model, dataloader, device, loss_fn, valid_loss, val_correct)
        else:
            return self.embeddings_validation(model, dataloader, device, loss_fn, valid_loss, val_correct)

    def classification_validation(self, model, dataloader, device, loss_fn, valid_loss, val_correct):
        with torch.no_grad():
            for images, labels in dataloader:
                images, labels = images.to(device), labels.to(device)
                output = model(images)
                loss = loss_fn(output, labels)
                valid_loss += loss.item() * images.size(0)
                scores, predictions = torch.max(output.data, 1)
                val_correct += (predictions == labels).sum().item()
        return valid_loss, val_correct

    def embeddings_validation(self, model, dataloader, device, loss_fn, valid_loss, val_correct):
        with torch.no_grad():
            for anchor_images,anchor_labels,positive_images,positive_labels, negative_images,negative_labels in dataloader:
                anchor_images, anchor_labels = anchor_images.to(device), anchor_labels.to(device)
                positive_images, positive_labels = positive_images.to(device), positive_labels.to(device)
                negative_images, negative_labels = negative_images.to(device), negative_labels.to(device)
                output1, output2, output3 = model(anchor_images, positive_images, negative_images)
                loss = loss_fn(output1, output2, output3)
                valid_loss += loss.item() #* anchor_images.size(0)
        return valid_loss, val_correct

    def train_loss_visalization(self, history):
        epochs = np.arange(1, len(history['train_loss'])+1)
        df = pd.DataFrame({
            'epoch': np.concatenate([epochs, epochs]),
            'loss': np.concatenate([history['train_loss'], history['test_loss']]),
            'type': ['train'] * len(history['train_loss']) + ['val'] * len(history['test_loss'])
        })

        # Use seaborn to create the plot
        plt.figure(figsize=(10, 6))
        sns.lineplot(x='epoch', y='loss', hue='type', data=df)
        plt.title('Training and Validation Loss Convergence')
        plt.xlabel('Epoch')
        plt.ylabel('Loss')
        # plt.show()
        # plt.savefig(
        #     f"/sise/home/omerhof/malware-classification/images/loss_converges_epoch_{len(history['train_loss'])}.png")
        plt.clf()



class classification_model(Finetune_trained_nets):

    def __init__(self, cfg):
        super(Finetune_trained_nets,self).__init__()
        self.cfg = cfg
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.model = classification_model_architecture(self.cfg)
        self.dataset = Classification_dataset(cfg.root_path)


class siamese_model(Finetune_trained_nets):
    def __init__(self, cfg):
        super(Finetune_trained_nets,self).__init__()
        self.cfg = cfg
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

        self.model = siamese_model_architecture(self.cfg)
        self.dataset = Triplet_loss_dataset(cfg.root_path)

