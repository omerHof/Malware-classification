
import torch
import torch.nn as nn
from torch.utils.data import DataLoader,SubsetRandomSampler
import torch.optim as optim
from configs.config_file import Classifier_config,Siamese_net_config
from sklearn.model_selection import KFold
import numpy as np
# from tqdm import tqdm
from tqdm import tqdm
from tqdm.notebook import tqdm as tqdm1
from Models.architectures import classification_model_architecture,siamese_model_architecture
from Data.data_util import Classification_dataset,Triplet_loss_dataset
import matplotlib.pyplot as plt
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score
from sklearn.decomposition import PCA

import imutils

class Finetune_trained_nets():
    def __init__(self, cfg):
        super().__init__()
        self.cfg = cfg

    def finetune_model(self):
        torch.manual_seed(42)
        splits = KFold(n_splits=self.cfg.finetune_params['kfold'], shuffle=True, random_state=42)
        history = {'train_loss': [], 'test_loss': [], 'train_acc': [], 'test_acc': []}

        for fold, (train_idx, val_idx) in enumerate(splits.split(np.arange(len(self.dataset)))):
            print('Fold {}'.format(fold + 1))

            train_sampler = SubsetRandomSampler(train_idx)
            test_sampler = SubsetRandomSampler(val_idx)
            train_loader = DataLoader(self.dataset, batch_size=self.cfg.loader_params['batch_size'],
                                      sampler=train_sampler, num_workers=self.cfg.loader_params['num_workers'])
            test_loader = DataLoader(self.dataset, batch_size=self.cfg.loader_params['batch_size'],
                                     sampler=test_sampler, num_workers=self.cfg.loader_params['num_workers'])

            model = self.model.model
            optim_params = model.parameters() if mode=='classification' else model.model.model.parameters()
            self.optimizer = optim.Adam(optim_params, lr=0.002)
            lr_scheduler = optim.lr_scheduler.StepLR(self.optimizer, step_size=10, gamma=0.1)
            for epoch in tqdm(range(self.cfg.finetune_params['epochs']),desc='Train net'):
                train_loss, train_correct, optimizer = self.train_epoch(model, self.device, train_loader, self.cfg.criterion, self.optimizer)
                test_loss, test_correct = self.valid_epoch(model, self.device, test_loader, self.cfg.criterion)

                train_loss = train_loss / len(train_loader.sampler)
                train_acc = train_correct / len(train_loader.sampler) * 100
                test_loss = test_loss / len(test_loader.sampler)
                test_acc = test_correct / len(test_loader.sampler) * 100

                if mode=='siamese':
                    test_acc = self.siamese_ml_model(model, train_loader, test_loader, "epoch")

                print(
                    "Epoch:{}/{} AVG Training Loss:{:.3f} AVG Test Loss:{:.3f} AVG Training Acc {:.2f} % AVG Test Acc {:.2f} %".format(
                        epoch + 1,
                        self.cfg.finetune_params['epochs'],
                        train_loss,
                        test_loss,
                        train_acc,
                        test_acc))
                history['train_loss'].append(train_loss)
                history['test_loss'].append(test_loss)
                history['train_acc'].append(train_acc)
                history['test_acc'].append(test_acc)
                lr_scheduler.step()

        if mode=='siamese':
            self.save_params(model, optimizer)
            accuracy = self.siamese_ml_model(model, train_loader, test_loader, "final")
            print(accuracy)

    def save_params(self, model, optimizer):
        print("save_params")
        torch.save({"model_state_dict": model.state_dict(),
                    "optimzier_state_dict": optimizer.state_dict()
                    }, "trained_model.pth")

    def siamese_ml_model(self, model, train_loader, test_loader, step):
        train_results = []
        train_labels = []
        test_results = []
        test_labels = []

        model.eval()
        with torch.no_grad():
            for anchor_images, anchor_labels, positive_images, positive_labels, negative_images, negative_labels in train_loader:
                train_results.append(anchor_images.to(self.device).cpu().numpy())
                train_labels.append(anchor_labels)
                train_results.append(positive_images.to(self.device).cpu().numpy())
                train_labels.append(positive_labels)
                train_results.append(negative_images.to(self.device).cpu().numpy())
                train_labels.append(negative_labels)

            for anchor_images, anchor_labels, positive_images, positive_labels, negative_images, negative_labels in test_loader:
                test_results.append(anchor_images.to(self.device).cpu().numpy())
                test_labels.append(anchor_labels)
                test_results.append(positive_images.to(self.device).cpu().numpy())
                test_labels.append(positive_labels)
                test_results.append(negative_images.to(self.device).cpu().numpy())
                test_labels.append(negative_labels)

        train_results = np.concatenate(train_results)
        train_labels = np.concatenate(train_labels)
        test_results = np.concatenate(test_results)
        if step == "final":
            self.visualization2(train_results, train_labels)
        test_labels = np.concatenate(test_labels)
        return self.ml_model(train_results, train_labels, test_results, test_labels)


    def ml_model(self, train_results, train_labels, test_results, test_labels):
        tree = XGBClassifier(seed=2020)
        train_images = []
        test_images = []
        for img in train_results:
            img = imutils.resize(img.transpose((1, 2, 0))[:, :, 0], width=28)
            img = list(img.reshape(-1))
            train_images.append(img)
        for img in test_results:
            img = imutils.resize(img.transpose((1, 2, 0))[:, :, 0], width=28)
            img = list(img.reshape(-1))
            test_images.append(img)
        tree.fit(train_images, train_labels)
        predictions = tree.predict(test_images)
        accuracy = accuracy_score(test_labels, predictions)
        return accuracy


    def visualization(self, train_results, labels):
        print("visualization")
        plt.figure(figsize=(15, 10), facecolor="azure")
        for label in np.unique(labels):
            tmp = train_results[labels == label]
            plt.scatter(tmp[:, 0], tmp[:, 1], label=label)

        plt.legend()
        plt.savefig("siamese.png")
        plt.show()

    def visualization2(self, train_results, labels):
        # Flatten the image array to (num_samples, num_features)
        num_samples = train_results.shape[0]
        num_features = np.prod(train_results.shape[1:])
        train_images_flattened = train_results.reshape(num_samples, num_features)

        # Perform PCA for dimensionality reduction to 2D
        pca = PCA(n_components=2)
        train_images_2d = pca.fit_transform(train_images_flattened)

        # Define the class labels and corresponding colors
        class_labels = ["Class A", "Class B", "Class C", "Class D", "Class E", "Class F","Class G", "Class H", "Class I"]  # Define the labels according to your dataset
        class_colors = ["red", "green", "blue", "pink", "brown", "yellow","orange", "grey", "purple"]  # Define the colors for each class

        # Plotting the data points
        fig = plt.figure(figsize=(8, 8))

        for i in range(len(class_labels)):
            # Filter the data points belonging to the current class
            class_indices = np.where(labels == i)[0]
            class_points = train_images_2d[class_indices]

            # Scatter plot the data points with the corresponding color
            plt.scatter(class_points[:, 0], class_points[:, 1], color=class_colors[i], label=class_labels[i])

        plt.xlabel("Principal Component 1")
        plt.ylabel("Principal Component 2")
        plt.title("Data Points Visualization")
        plt.legend()
        plt.savefig("siamese.png")
        plt.show()


    def train_epoch(self,model, device, dataloader, loss_fn, optimizer):
        train_loss, train_correct = 0.0, 0
        model.train()
        if self.cfg.mode=='classification':
            return self.training_loop_classification(model, device, dataloader, loss_fn, optimizer,train_loss,train_correct)
        else:
            return self.training_loop_embedding(model, device, dataloader, loss_fn, optimizer,train_loss,train_correct)


    def training_loop_classification(self,model, device, dataloader, loss_fn, optimizer,train_loss,train_correct):
        for images, labels in dataloader:
            images, labels = images.to(device), labels.to(device)
            optimizer.zero_grad()
            output = model(images)
            loss = loss_fn(output, labels)
            loss.backward()
            optimizer.step()
            train_loss += loss.item() * images.size(0)
            scores, predictions = torch.max(output.data, 1)
            train_correct += (predictions == labels).sum().item()
        return train_loss, train_correct, optimizer

    def training_loop_embedding(self, model, device, dataloader, loss_fn, optimizer, train_loss, train_correct):
        data = []
        for anchor_images,anchor_labels,positive_images,positive_labels, negative_images,negative_labels in dataloader:
            anchor_images,anchor_labels = anchor_images.to(device), anchor_labels.to(device)
            positive_images,positive_labels = positive_images.to(device), positive_labels.to(device)
            negative_images,negative_labels = negative_images.to(device), negative_labels.to(device)
            optimizer.zero_grad()
            output1, output2, output3 = model(anchor_images,positive_images,negative_images)
            loss = loss_fn(output1, output2, output3)
            loss.backward()
            optimizer.step()

            train_loss += loss.item() * anchor_images.size(0)
            # implement here KNN to classify the embedding vector
        return train_loss, train_correct, optimizer

    def valid_epoch(self,model, device, dataloader, loss_fn):
        valid_loss, val_correct = 0.0, 0
        model.eval()
        if self.cfg.mode == 'classification':
            return self.classification_validation(model, dataloader, device, loss_fn, valid_loss, val_correct)
        else:
            return self.embeddings_validation(model, dataloader, device, loss_fn, valid_loss, val_correct)

    def classification_validation(self, model, dataloader, device, loss_fn, valid_loss, val_correct):
        with torch.no_grad():
            for images, labels in dataloader:
                images, labels = images.to(device), labels.to(device)
                output = model(images)
                loss = loss_fn(output, labels)
                valid_loss += loss.item() * images.size(0)
                scores, predictions = torch.max(output.data, 1)
                val_correct += (predictions == labels).sum().item()
        return valid_loss, val_correct

    def embeddings_validation(self, model, dataloader, device, loss_fn, valid_loss, val_correct):
        with torch.no_grad():
            for anchor_images,anchor_labels,positive_images,positive_labels, negative_images,negative_labels in dataloader:
                anchor_images, anchor_labels = anchor_images.to(device), anchor_labels.to(device)
                positive_images, positive_labels = positive_images.to(device), positive_labels.to(device)
                negative_images, negative_labels = negative_images.to(device), negative_labels.to(device)
                output1, output2, output3 = model(anchor_images, positive_images, negative_images)
                loss = loss_fn(output1, output2, output3)
                valid_loss += loss.item() * anchor_images.size(0)
        return valid_loss, val_correct


class classification_model(Finetune_trained_nets):

    def __init__(self, cfg):
        super(Finetune_trained_nets,self).__init__()
        self.cfg = cfg
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.model = classification_model_architecture(self.cfg)
        self.dataset = Classification_dataset(cfg.root_path)


class siamese_model(Finetune_trained_nets):
    def __init__(self, cfg):
        super(Finetune_trained_nets,self).__init__()
        self.cfg = cfg
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

        self.model = siamese_model_architecture(self.cfg)
        self.dataset = Triplet_loss_dataset(cfg.root_path)


if __name__ == '__main__':
    print('check')
    # mode = 'classification'
    mode = 'siamese'
    if mode=='classification':
        cfg = Classifier_config()
        ftn = classification_model(cfg)
    else:
        cfg = Siamese_net_config()
        ftn = siamese_model(cfg)
    ftn.finetune_model()
