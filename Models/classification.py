
import torch
import torch.nn as nn
from torch.utils.data import DataLoader,SubsetRandomSampler
import torch.optim as optim
from configs.config_file import Classifier_config,Siamese_net_config
from sklearn.model_selection import KFold
import numpy as np
# from tqdm import tqdm
from tqdm import tqdm
from tqdm.notebook import tqdm as tqdm1
from Models.architectures import classification_model_architecture,siamese_model_architecture
from Data.data_util import Classification_dataset,Triplet_loss_dataset
import matplotlib.pyplot as plt
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score
from sklearn.decomposition import PCA
import pandas as pd

import imutils

class Finetune_trained_nets():
    def __init__(self, cfg):
        super().__init__()
        self.cfg = cfg

    def finetune_model(self):
        torch.manual_seed(42)
        splits = KFold(n_splits=self.cfg.finetune_params['kfold'], shuffle=True, random_state=42)
        history = {'train_loss': [], 'test_loss': [], 'train_acc': [], 'test_acc': []}

        for fold, (train_idx, val_idx) in enumerate(splits.split(np.arange(len(self.dataset)))):
            print('Fold {}'.format(fold + 1))
            if mode == 'siamese':
                train_siamese, valid_siamese, train_ml = np.split(train_idx, np.cumsum(np.floor(np.multiply([0.15, 0.05, 0.8], len(train_idx))).astype(int))[:-1])
                train_siamese_sampler = SubsetRandomSampler(train_siamese)
                valid_siamese_sampler = SubsetRandomSampler(valid_siamese)
                train_ml_sampler = SubsetRandomSampler(train_ml)
                test_sampler = SubsetRandomSampler(val_idx)
                train_loader = DataLoader(self.dataset, batch_size=self.cfg.loader_params['batch_size'],
                                          sampler=train_siamese_sampler, num_workers=self.cfg.loader_params['num_workers'])
                test_loader = DataLoader(self.dataset, batch_size=self.cfg.loader_params['batch_size'],
                                         sampler=valid_siamese_sampler, num_workers=self.cfg.loader_params['num_workers'])
                train_ml_loader = DataLoader(self.dataset, batch_size=self.cfg.loader_params['batch_size'],
                                          sampler=train_ml_sampler, num_workers=self.cfg.loader_params['num_workers'])
                test_ml_loader = DataLoader(self.dataset, batch_size=self.cfg.loader_params['batch_size'],
                                         sampler=test_sampler, num_workers=self.cfg.loader_params['num_workers'])
            else:
                train_sampler = SubsetRandomSampler(train_idx)
                test_sampler = SubsetRandomSampler(val_idx)
                train_loader = DataLoader(self.dataset, batch_size=self.cfg.loader_params['batch_size'],
                                          sampler=train_sampler, num_workers=self.cfg.loader_params['num_workers'])
                test_loader = DataLoader(self.dataset, batch_size=self.cfg.loader_params['batch_size'],
                                         sampler=test_sampler, num_workers=self.cfg.loader_params['num_workers'])

            model = self.model.model
            optim_params = model.parameters() if mode=='classification' else model.model.model.parameters()
            self.optimizer = optim.Adam(optim_params, lr=0.0001)
            lr_scheduler = optim.lr_scheduler.StepLR(self.optimizer, step_size=10, gamma=0.1)
            for epoch in tqdm(range(self.cfg.finetune_params['epochs']),desc='Train net'):
                if mode == 'siamese':
                    train_loss, train_correct, optimizer, model = self.train_epoch(model, self.device, train_loader, self.cfg.criterion, self.optimizer)
                    test_loss, test_correct = self.valid_epoch(model, self.device, test_loader, self.cfg.criterion)
                else:
                    train_loss, train_correct, optimizer, model = self.train_epoch(model, self.device, train_loader, self.cfg.criterion, self.optimizer)
                    test_loss, test_correct = self.valid_epoch(model, self.device, test_loader, self.cfg.criterion)

                train_loss = train_loss / len(train_loader.sampler)
                train_acc = train_correct / len(train_loader.sampler) * 100
                test_loss = test_loss / len(test_loader.sampler)
                test_acc = test_correct / len(test_loader.sampler) * 100

                if mode=='siamese' and epoch%5==0:
                    accuracy = self.siamese_ml_model(model, train_ml_loader, test_ml_loader, fold, epoch)
                    print(accuracy)

                print(
                    "Epoch:{}/{} AVG Training Loss:{:.3f} AVG Test Loss:{:.3f} AVG Training Acc {:.2f} % AVG Test Acc {:.2f} %".format(
                        epoch + 1,
                        self.cfg.finetune_params['epochs'],
                        train_loss,
                        test_loss,
                        train_acc,
                        test_acc))
                history['train_loss'].append(train_loss)
                history['test_loss'].append(test_loss)
                history['train_acc'].append(train_acc)
                history['test_acc'].append(test_acc)
                lr_scheduler.step()

            if mode=='siamese':
                self.save_params(model, optimizer)
                accuracy = self.siamese_ml_model(model, train_ml_loader, test_ml_loader, fold, -1)
                print(accuracy)

    def save_params(self, model, optimizer):
        print("save_params")
        torch.save({"model_state_dict": model.state_dict(),
                    "optimzier_state_dict": optimizer.state_dict()
                    }, "trained_model.pth")

    def siamese_ml_model(self, siamese_model, train_loader, test_loader, fold, epoch):
        # model return the embedding from the images
        # model.eval()
        def create_embedding_lists(data_loader):
            embeddings = []
            lables = []
            with torch.no_grad():
                for anchor_images, anchor_labels, positive_images, positive_labels, negative_images, negative_labels in data_loader:
                    anchor_embed, positive_embed, negative_embed = siamese_model(anchor_images.to(self.device), positive_images.to(self.device), negative_images.to(self.device))
            #         embeddings.append(np.mean(anchor_embed.cpu().numpy(), axis=0))
            #         embeddings.append(np.mean(positive_embed.cpu().numpy(), axis=0))
            #         embeddings.append(np.mean(negative_embed.cpu().numpy(), axis=0))
            #         lables.append(anchor_labels.cpu().numpy()[0])
            #         lables.append(positive_labels.cpu().numpy()[0])
            #         lables.append(negative_labels.cpu().numpy()[0])
            # return np.stack(embeddings), np.stack(lables)
                    embeddings.append(anchor_embed.cpu().numpy())
                    embeddings.append(positive_embed.cpu().numpy())
                    embeddings.append(negative_embed.cpu().numpy())
                    lables.append(anchor_labels.cpu().numpy())
                    lables.append(positive_labels.cpu().numpy())
                    lables.append(negative_labels.cpu().numpy())
            return np.vstack(embeddings), np.hstack(lables)

        train_embedding, train_labels = create_embedding_lists(train_loader)
        test_embedding, test_labels = create_embedding_lists(test_loader)

        # if step == "final":
        #     self.visualization2(train_results, train_labels)
        # test_labels = c(test_labels)
        return self.ml_model(train_embedding, train_labels, test_embedding, test_labels, fold, epoch)


    def ml_model(self, train_images, train_labels, test_images, test_labels, fold, epoch):
        from sklearn.neural_network import MLPClassifier
        mlp = MLPClassifier(hidden_layer_sizes=(16, 16, 16), activation='relu', solver='adam', max_iter=1000)

        tree = XGBClassifier(seed=2020)
        from sklearn.preprocessing import LabelEncoder
        le = LabelEncoder()
        train_labels = le.fit_transform(train_labels)

        tree.fit(pd.DataFrame(train_images), pd.Series(train_labels))
        predictions = tree.predict(pd.DataFrame(test_images))
        predictions = le.inverse_transform(predictions)
        accuracy = accuracy_score(test_labels, predictions)
        self.visualization2(train_images, train_labels, fold, epoch)
        return accuracy


    def visualization(self, train_results, labels, fold, epoch):
        from mpl_toolkits.mplot3d import Axes3D
        print("visualization_"+str(fold+1)+"_"+str(epoch))
        # plt.figure(figsize=(15, 10), facecolor="azure")
        fig = plt.figure(figsize=(15, 10), facecolor="azure")
        ax = fig.add_subplot(111, projection='3d')
        for label in np.unique(labels):
            tmp = train_results[labels == label]
            plt.scatter(tmp[:, 0], tmp[:, 1], tmp[:, 2], label=label, alpha=0.7)

        # plt.legend()
        ax.legend()
        ax.set_xlabel('X')
        ax.set_ylabel('Y')
        ax.set_zlabel('Z')
        plt.savefig("siamese_"+str(fold+1)+"_"+str(epoch)+".png")
        plt.show()

    def visualization2(self, train_results, labels, fold, epoch):
        # Flatten the image array to (num_samples, num_features)
        print("visualization_" + str(fold + 1) + "_" + str(epoch))
        num_samples = train_results.shape[0]
        num_features = np.prod(train_results.shape[1:])
        train_images_flattened = train_results.reshape(num_samples, num_features)

        # Perform PCA for dimensionality reduction to 2D
        pca = PCA(n_components=2)
        train_images_2d = pca.fit_transform(train_images_flattened)

        # Define the class labels and corresponding colors
        class_labels = ["Class A", "Class B", "Class C", "Class D", "Class E", "Class F","Class G", "Class H", "Class I"]  # Define the labels according to your dataset
        class_colors = ["red", "green", "blue", "pink", "brown", "yellow","orange", "grey", "purple"]  # Define the colors for each class

        # Plotting the data points
        fig = plt.figure(figsize=(8, 8))

        for i in range(len(class_labels)):
            # Filter the data points belonging to the current class
            class_indices = np.where(labels == i)[0]
            class_points = train_images_2d[class_indices]

            # Scatter plot the data points with the corresponding color
            plt.scatter(class_points[:, 0], class_points[:, 1], color=class_colors[i], label=class_labels[i])

        plt.xlabel("Principal Component 1")
        plt.ylabel("Principal Component 2")
        plt.title("Data Points Visualization")
        plt.legend()
        plt.savefig("images/siamese_"+str(fold+1)+"_"+str(epoch)+".png")
        plt.show()


    def train_epoch(self,model, device, dataloader, loss_fn, optimizer):
        train_loss, train_correct = 0.0, 0
        model.train()
        if self.cfg.mode=='classification':
            return self.training_loop_classification(model, device, dataloader, loss_fn, optimizer,train_loss,train_correct)
        else:
            return self.training_loop_embedding(model, device, dataloader, loss_fn, optimizer,train_loss,train_correct)


    def training_loop_classification(self,model, device, dataloader, loss_fn, optimizer,train_loss,train_correct):
        for images, labels in dataloader:
            images, labels = images.to(device), labels.to(device)
            optimizer.zero_grad()
            output = model(images)
            loss = loss_fn(output, labels)
            loss.backward()
            optimizer.step()
            train_loss += loss.item() * images.size(0)
            scores, predictions = torch.max(output.data, 1)
            train_correct += (predictions == labels).sum().item()
        return train_loss, train_correct, optimizer

    def training_loop_embedding(self, model, device, dataloader, loss_fn, optimizer, train_loss, train_correct):
        data = []
        for anchor_images,anchor_labels,positive_images,positive_labels, negative_images,negative_labels in dataloader:
            anchor_images,anchor_labels = anchor_images.to(device), anchor_labels.to(device)
            positive_images,positive_labels = positive_images.to(device), positive_labels.to(device)
            negative_images,negative_labels = negative_images.to(device), negative_labels.to(device)
            optimizer.zero_grad()
            output1, output2, output3 = model(anchor_images,positive_images,negative_images)
            loss = loss_fn(output1, output2, output3)
            loss.backward()
            optimizer.step()

            train_loss += loss.item() * anchor_images.size(0)
            # implement here KNN to classify the embedding vector
        return train_loss, train_correct, optimizer, model

    def valid_epoch(self,model, device, dataloader, loss_fn):
        valid_loss, val_correct = 0.0, 0
        model.eval()
        if self.cfg.mode == 'classification':
            return self.classification_validation(model, dataloader, device, loss_fn, valid_loss, val_correct)
        else:
            return self.embeddings_validation(model, dataloader, device, loss_fn, valid_loss, val_correct)

    def classification_validation(self, model, dataloader, device, loss_fn, valid_loss, val_correct):
        with torch.no_grad():
            for images, labels in dataloader:
                images, labels = images.to(device), labels.to(device)
                output = model(images)
                loss = loss_fn(output, labels)
                valid_loss += loss.item() * images.size(0)
                scores, predictions = torch.max(output.data, 1)
                val_correct += (predictions == labels).sum().item()
        return valid_loss, val_correct

    def embeddings_validation(self, model, dataloader, device, loss_fn, valid_loss, val_correct):
        with torch.no_grad():
            for anchor_images,anchor_labels,positive_images,positive_labels, negative_images,negative_labels in dataloader:
                anchor_images, anchor_labels = anchor_images.to(device), anchor_labels.to(device)
                positive_images, positive_labels = positive_images.to(device), positive_labels.to(device)
                negative_images, negative_labels = negative_images.to(device), negative_labels.to(device)
                output1, output2, output3 = model(anchor_images, positive_images, negative_images)
                loss = loss_fn(output1, output2, output3)
                valid_loss += loss.item() * anchor_images.size(0)
        return valid_loss, val_correct


class classification_model(Finetune_trained_nets):

    def __init__(self, cfg):
        super(Finetune_trained_nets,self).__init__()
        self.cfg = cfg
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.model = classification_model_architecture(self.cfg)
        self.dataset = Classification_dataset(cfg.root_path)


class siamese_model(Finetune_trained_nets):
    def __init__(self, cfg):
        super(Finetune_trained_nets,self).__init__()
        self.cfg = cfg
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

        self.model = siamese_model_architecture(self.cfg)
        self.dataset = Triplet_loss_dataset(cfg.root_path)


if __name__ == '__main__':
    print('check')
    # mode = 'classification'
    mode = 'siamese'
    if mode=='classification':
        cfg = Classifier_config()
        ftn = classification_model(cfg)
    else:
        cfg = Siamese_net_config()
        ftn = siamese_model(cfg)
    ftn.finetune_model()
